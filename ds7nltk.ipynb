{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7: Text Analytics\n",
    "# Aim:\n",
    "# 1. Apply Text Preprocessing: Tokenization, POS Tagging, Stopword Removal, Stemming, Lemmatization.\n",
    "# 2. Represent the document using TF (Term Frequency) and IDF (Inverse Document Frequency).\n",
    "\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # For breaking text into words\n",
    "from nltk.corpus import stopwords        # For getting a list of common stopwords\n",
    "from nltk.stem import PorterStemmer      # For reducing words to their root form\n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatizing words to dictionary base form\n",
    "from nltk import pos_tag                 # For identifying parts of speech\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For calculating TF-IDF\n",
    "\n",
    "# Download necessary resources (only required once)\n",
    "nltk.download('punkt')                          # Tokenizer\n",
    "nltk.download('stopwords')                      # Stopword list\n",
    "nltk.download('wordnet')                        # WordNet lemmatizer dictionary\n",
    "nltk.download('averaged_perceptron_tagger')     # POS tagger\n",
    "\n",
    "# Step 1: Define a sample text document\n",
    "text = \"Text Analytics is the process of extracting meaningful information from text data.\"\n",
    "\n",
    "# Step 2: Tokenization – breaking the text into individual words (tokens)\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\nTokens (individual words):\", tokens)\n",
    "\n",
    "# Step 3: Part-of-Speech (POS) Tagging – tagging each word with its grammatical role\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags (e.g., noun, verb):\", pos_tags)\n",
    "\n",
    "# Step 4: Stopword Removal – filtering out common words that add little value to meaning\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nAfter Stopword Removal (important words only):\", filtered_tokens)\n",
    "\n",
    "# Step 5: Stemming – reducing words to their root (may not always be a real word)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nStemmed Words (rough root form):\", stemmed_words)\n",
    "\n",
    "# Step 6: Lemmatization – reducing words to their dictionary form (more accurate than stemming)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nLemmatized Words (proper root form):\", lemmatized_words)\n",
    "\n",
    "# Step 7: TF-IDF Calculation\n",
    "# TF = how often a word occurs in a document\n",
    "# IDF = how rare a word is across all documents\n",
    "# TF-IDF = importance of a word in a specific document relative to others\n",
    "\n",
    "documents = [text]  # A list of documents (we only have 1 here, but more can be added)\n",
    "tfidf = TfidfVectorizer()  # Create the TF-IDF transformer\n",
    "tfidf_matrix = tfidf.fit_transform(documents)  # Apply it to the documents\n",
    "print(\"\\nTF-IDF Matrix (numerical importance of words):\\n\", tfidf_matrix.toarray())\n",
    "print(\"\\nTF-IDF Vocabulary (word to index mapping):\\n\", tfidf.vocabulary_)\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ VIVA QUESTIONS (in comments)\n",
    "# -------------------------------\n",
    "\n",
    "# Q1: What is Tokenization?\n",
    "# A: It breaks text into smaller pieces like words or sentences. For example: \"Text Analytics\" -> [\"Text\", \"Analytics\"]\n",
    "\n",
    "# Q2: What is POS Tagging?\n",
    "# A: It assigns parts of speech like noun, verb, etc., to each token to understand sentence structure.\n",
    "\n",
    "# Q3: What is Stopword Removal?\n",
    "# A: It removes commonly used words (like \"is\", \"the\") that don’t add much meaning to analysis.\n",
    "\n",
    "# Q4: What is Stemming?\n",
    "# A: It cuts words down to their base or root (e.g., \"running\" becomes \"run\"), but might not be valid words.\n",
    "\n",
    "# Q5: What is Lemmatization?\n",
    "# A: It transforms words into their dictionary base forms using proper context (e.g., \"running\" → \"run\").\n",
    "\n",
    "# Q6: Stemming vs Lemmatization?\n",
    "# A: Stemming is rule-based and rough. Lemmatization is dictionary-based and more accurate.\n",
    "\n",
    "# Q7: What is TF-IDF?\n",
    "# A: It weighs the importance of words in a document relative to all documents in the corpus.\n",
    "\n",
    "# Q8: Why use TF-IDF?\n",
    "# A: To convert text into numbers that reflect word importance for use in machine learning models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
